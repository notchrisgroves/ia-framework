# Phase 3: QA REVIEW

## ðŸš¨ STEP 0: VOICE CHECK (BLOCKING - BEFORE ALL OTHER QA)

**This check MUST pass before ANY content review.**

### Voice Verification Checklist

Read the draft and answer EACH question:

**1. HOOK TEST - Does the opening grab attention?**
- [ ] First sentence is a bold question OR bold statement
- [ ] NOT narrative ("In 2024, I started...")
- [ ] NOT passive ("This post will cover...")

**If FAIL:** STOP. Rewrite opening with: "What if [bold claim]?" or "**[Bold statement that challenges assumptions]**"

**2. CODE SEASONING TEST - Is code illustrative, not exhaustive?**
- [ ] First code block is under 15 lines
- [ ] No code block exceeds 25 lines
- [ ] Code blocks have context explaining WHY, not just WHAT
- [ ] **NO LINE EXCEEDS 80 CHARACTERS** in code blocks (prevents horizontal scroll)

**If FAIL:** STOP. Trim code blocks. Link to full implementation, don't paste it.
**If lines too long:** Break into vertical flow diagram, wrap text, or split into multiple blocks.

**3. WARMTH TEST - Are diagrams introduced with analogies?**
- [ ] Every diagram/ASCII art has a "think of it like..." before it
- [ ] Technical concepts have real-world analogies
- [ ] No cold technical explanation without warmth

**If FAIL:** STOP. Add analogy before each diagram.

**4. "SO WHAT" TEST - Does every section answer why it matters?**
- [ ] Each section starts with a problem or benefit
- [ ] Not just "here's how X works" but "X solves [problem], here's how"
- [ ] Mission language present ("accessible", "deploy yourself", "expertise")

**If FAIL:** STOP. Add problem/benefit framing to each section.

**5. GOLD STANDARD TEST - Compare to reference**
- [ ] Compare to `archive/blog-v3/published/20251203-ia-intro/final.md`
- [ ] Does draft match that voice quality?
- [ ] Would the old intro author approve this draft?

**If FAIL:** STOP. Rewrite sections that don't match gold standard voice.

---

### â›” VOICE GATE

**ALL 5 CHECKS MUST PASS BEFORE PROCEEDING TO CONTENT QA.**

If ANY check fails:
1. Document which check(s) failed
2. Revise draft to fix violations
3. Re-run voice check
4. Only proceed when ALL 5 pass

**Voice Rating (1-5):**
- 5: Matches gold standard perfectly
- 4: Minor tweaks needed (acceptable to proceed)
- 3: Significant voice issues (BLOCK)
- 2: Major rewrite needed (BLOCK)
- 1: Wrong voice entirely (BLOCK)

**Minimum to proceed: Voice Rating >= 4**

---

## ðŸš¨ CRITICAL RULES

**Before starting content QA (after voice passes):**
1. **Verify Phase 2 Complete** - `draft.md` MUST exist with valid frontmatter
2. **This phase is MANDATORY** - NEVER skip QA review
3. **Voice Check PASSED** - Step 0 must be complete

**You MUST:**
- Pass Step 0 Voice Check (rating >= 4)
- Run automated dual-model QA: `bun run skills/writer/scripts/blog-workflow.ts qa {slug}`
- Document ALL feedback in qa-review.json (auto-generated)
- Achieve rating >= 4.0 (avg of Sonnet + Grok) before proceeding
- Revise draft if rating < 4.0
- **CHECK FOR HARDCODED COUNTS** - This is a blocking issue
- **VERIFY ALL SOURCES** - Web search to confirm citations exist

**NEVER:**
- âŒ Skip voice check (Step 0)
- âŒ Skip QA review
- âŒ Proceed with rating < 5
- âŒ Self-approve without external model validation
- âŒ Ignore QA feedback
- âŒ Allow hardcoded counts (e.g., "43 tools", "17 skills", "8 agents")

---

## Required Output Files

You MUST create in `blog/YYYY-MM-DD-{slug}/`:

- [ ] `qa-review.json` - Complete QA feedback with ratings
- [ ] `draft.md` - Revised if QA identified issues (or `draft-v2.md`)
- [ ] `metadata.json` - Updated with phase: "qa", qa_rating

---

## Step 1: Check for Hardcoded Counts (BLOCKING)

**BEFORE any other QA, scan draft for hardcoded counts:**

```
Search for patterns like:
- "\d+ tools" (e.g., "43 tools", "17 tools")
- "\d+ skills" (e.g., "18 skills", "7 skills")
- "\d+ agents" (e.g., "4 agents", "8 agents")
- "\d+ commands" (e.g., "15 commands")
- "Total: \d+"
```

**If ANY hardcoded counts found:**
1. STOP - This is a blocking issue
2. Replace with qualifiers: "Multiple tools", "Various skills", "Specialized agents"
3. Re-scan to confirm removal
4. Then proceed with QA

**This check is MANDATORY per CLAUDE.md Critical Requirements.**

---

## Step 1b: Citation Style Verification (BLOCKING)

**Check for proper hybrid citation style:**

### Inline Links Check
Scan for key claims that REQUIRE inline links:
- [ ] Statistics have inline links (e.g., "[9.5% accuracy loss](url)")
- [ ] CVE numbers are linked (e.g., "[CVE-2025-XXXX](url)")
- [ ] Research study citations are linked inline
- [ ] Not over-linked (general statements don't need links)

### Sources Section Check
- [ ] `## Sources` section exists at end of post
- [ ] Sources are categorized by topic
- [ ] All inline-linked sources also appear in Sources section
- [ ] URLs are valid markdown links, not raw URLs

**If ANY citation check fails:**
1. STOP - This is a blocking issue
2. Add missing inline links for key claims
3. Fix Sources section formatting
4. Re-verify before proceeding

---

## Step 1c: Source Verification (BLOCKING)

**Verify that cited sources actually exist:**

### Web Search Verification
For each key claim with a citation:
1. Web search to confirm the source exists
2. Verify the URL is accessible
3. Confirm the claim matches what the source says

### Common Issues to Catch:
- âŒ Fabricated arXiv paper IDs (e.g., made-up paper numbers)
- âŒ Non-existent CVE numbers
- âŒ Incomplete URLs (just domain, not full article path)
- âŒ Claims that don't match the cited source
- âŒ Outdated sources with newer data available

### Verification Checklist:
- [ ] All arXiv papers verified via `arxiv.org/abs/[ID]`
- [ ] All CVEs verified via NIST NVD or security vendor blogs
- [ ] All statistics traced to primary sources
- [ ] All executive quotes verified with news sources
- [ ] URLs are complete paths (not just domain names)

**If ANY source fails verification:**
1. STOP - This is a blocking issue
2. Research to find correct source or update claim
3. Replace fabricated citations with verified ones
4. Re-verify before proceeding

**This step prevents publishing content with hallucinated citations.**

---

## Step 1d: Visibility Classification Validation (BLOCKING)

**Verify visibility matches content type using classification rules:**

### Classification Check

Apply decision tree from 02-DRAFT.md:

```
IF title contains ["Deep Dive", "Building", "Creating", "Implementing",
                   "Extending", "Custom", "Internals", "Tutorial"]
   OR category == "implementation"
   â†’ EXPECTED: "paid"

ELSE IF title contains ["Guide", "Setup", "Hardening", "Methodology",
                        "Foundations", "Lab", "Professional", "Infrastructure"]
   OR category in ["security", "infrastructure"]
   â†’ EXPECTED: "members"

ELSE IF title contains ["Architecture", "Overview", "Introduction", "Analysis",
                        "Comparison", "Announcement", "Reality Check", "Companion"]
   OR category in ["commentary", "announcement", "framework", "tools"]
   â†’ EXPECTED: "public"

ELSE â†’ EXPECTED: "members"
```

### Validation:
- [ ] Frontmatter `visibility` matches expected tier
- [ ] If mismatch: Check for `visibility_override: true` in frontmatter
- [ ] If override present: Verify `visibility_reason` explains why

**If mismatch WITHOUT override:**
1. WARN - "Visibility mismatch: expected [X], got [Y]"
2. Either fix visibility OR add override with reason
3. Re-validate before proceeding

**Example override:**
```yaml
visibility: "public"
visibility_override: true
visibility_reason: "Launch promotion for new feature"
```

---

## Step 1e: CTA Footer Check (BLOCKING)

**Verify membership CTA exists before Sources section:**

- [ ] Post contains CTA text with subscription link
- [ ] CTA appears BEFORE the `## Sources` section
- [ ] Link points to `https://yourblog.ghost.io/#/portal/signup`

**Expected CTA format:**
```markdown
---

*Found this helpful? [Subscribe free](https://yourblog.ghost.io/#/portal/signup)...*

---

## Sources
```

**If CTA missing:**
1. STOP - This is a blocking issue
2. Add CTA footer before Sources section
3. Re-verify before proceeding

---

## Step 2: Sonnet Structured Review (NATIVE - No API Cost)

**Claude Code performs this review directly (no OpenRouter call):**

Review draft.md against these criteria, rating each 1-5:

1. **COMPLETENESS** - All claims backed by evidence/citations, code examples present, sources section exists
2. **TECHNICAL ACCURACY** - Claims factually correct, statistics have inline citations, no hallucinated references
3. **STRUCTURE & CLARITY** - Strong hook in opening, clear H2/H3 hierarchy, practical takeaways
4. **STYLE COMPLIANCE** - No AI clichÃ©s, no corporate buzzwords, professional tone, no hardcoded counts
5. **CITATION VALIDATION** - Inline links for key statistics, sources section at end
6. **VISIBILITY & CTA** - Visibility matches classification rules, CTA footer present before Sources

**Provide OVERALL RATING [1-5] and list specific issues with line references.**

**Record your Sonnet rating (1-5) for Step 3.**

---

## Step 3: Grok Adversarial Review (OpenRouter)

**EXACT COMMAND:**
```bash
bun run skills/writer/scripts/blog-workflow.ts qa YYYY-MM-DD-{slug} [sonnet_rating]
```

**Example:** `bun run skills/writer/scripts/blog-workflow.ts qa 2025-12-20-post-title 4`

**What happens:**
1. Calls Grok via OpenRouter for adversarial review
2. Cross-validates with your Sonnet rating
3. Saves `qa-review.json` with both reviews

**Gate:** Average of Sonnet + Grok ratings must be >= 4.0

---

## Step 4: Rating Scale

| Rating | Meaning | Action |
|--------|---------|--------|
| 5 | Exceptional | Publish immediately |
| 4 | Strong | Minor edits, then publish |
| 3 | Needs work | Major revision required |
| 2 | Weak | Significant rewrite |
| 1 | Poor | Start over |

---

## Step 5: Process QA Feedback

**If rating = 5:**
- Perfect! Proceed to Phase 4
- No further revisions needed

**If rating < 5:**
1. STOP - Do not proceed
2. Review ALL feedback in qa-review.json
3. **ADDRESS EACH GROK CONCERN INDIVIDUALLY** (see Step 5a)
4. Apply corrections thoroughly
5. Save revised draft (draft-v2.md, draft-v3.md, etc.)
6. Re-run QA review
7. **Repeat until rating = 5/5**

**This may take multiple iterations - that's expected and normal.**

---

## Step 5a: Address Each Grok Concern (MANDATORY)

ðŸš¨ **NO OVERRIDES ALLOWED WITHOUT ADDRESSING EACH CONCERN**

When Grok identifies issues, you MUST address EACH ONE with one of these responses:

### For Each Issue in qa-review.json:

**1. OVERSTATEMENTS** - Fix EVERY one:
- Soften absolute language ("always" â†’ "often", "cannot" â†’ "typically difficult to")
- Add qualifiers where evidence is thin
- Remove claims that exceed the evidence

**2. LOGICAL GAPS** - Fill or acknowledge EVERY one:
- Add missing reasoning steps
- Explain causal chains explicitly
- OR acknowledge limitations in the text

**3. POTENTIAL HALLUCINATIONS** - Verify or remove EVERY one:
- Web search to verify the claim
- If verified: add proper citation
- If NOT verified: REMOVE the claim entirely

**4. MISSING CONTEXT** - Add EVERY missing element:
- Edge cases Grok identified
- Counterarguments that should be addressed
- Important caveats omitted

**5. WEAK EVIDENCE** - Strengthen or caveat EVERY one:
- Add citations if available
- Soften claims if evidence thin
- Mark as "author's framework/opinion" if original

### Documentation Required

Create `blog/YYYY-MM-DD-{slug}/grok-response.md` with:

```markdown
# Grok Concern Resolution

## Overstatements Addressed
1. "[Original claim]" â†’ "[Revised claim]" (Line X)
2. ...

## Logical Gaps Filled
1. "[Gap identified]" â†’ "[How addressed]" (Line X)
2. ...

## Hallucinations Resolved
1. "[Claim]" â†’ VERIFIED with [source] OR REMOVED
2. ...

## Context Added
1. "[Missing context]" â†’ "[Added where]" (Line X)
2. ...

## Evidence Strengthened
1. "[Weak claim]" â†’ "[How strengthened]" (Line X)
2. ...
```

**â›” CANNOT PROCEED UNTIL grok-response.md EXISTS AND ALL CONCERNS ADDRESSED**

---

## â›” HARD GATE (MANDATORY)

**Cannot proceed to Phase 4 (VISUALS) unless:**
- [ ] `qa-review.json` exists
- [ ] Rating is 5/5 (PERFECT score required)
- [ ] ALL identified issues addressed
- [ ] `metadata.json` updated with qa_rating = 5

ðŸš¨ **If rating < 5: STOP. Revise. Re-run QA. Repeat until 5/5.**

This gate is NON-NEGOTIABLE. We publish ONLY exceptional content.
Multiple QA iterations are expected - this is normal.

---

## qa-review.json Format

```json
{
  "reviewed_at": "2025-12-20T...",
  "title": "Post Title",
  "reviewers": ["sonnet", "grok"],
  "models": {
    "sonnet": "claude-sonnet-4 (native)",
    "grok": "x-ai/grok-4-fast"
  },
  "ratings": {
    "sonnet": 4,
    "grok": 4,
    "average": 4.0,
    "agreement": "high"
  },
  "sonnet_review": "Done natively by Claude Code",
  "grok_review": "## OVERSTATEMENTS: 0 found...",
  "tokens_used": {
    "sonnet": 0,
    "grok": 1800,
    "total": 1800
  },
  "gate_passed": true
}
```

---

## Checkpoint Output

**Show user:**
```
âœ… PHASE 3 COMPLETE: QA Review
Rating: [X]/5
Reviewers: Sonnet, Grok
Feedback: [summary of key points]
Revisions made: [list if any]

Gate: PASSED âœ“ (rating = 5/5)
â†’ Ready for Phase 4: VISUALS
```

**If rating < 5:**
```
â›” PHASE 3 BLOCKED: QA Review
Rating: [X]/5 (REQUIRES 5/5)
Iteration: [N] of ?
Required changes:
1. [issue]
2. [issue]
...

Action: Revising draft based on feedback...
[After revision, re-run QA - repeat until 5/5]
```

---

**Next Phase:** Load `prompts/04-VISUALS.md`
