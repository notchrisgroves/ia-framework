#!/usr/bin/env python3
"""
Test Fix in Isolated Environment

Tests generated fix with:
- Syntax validation
- Vulnerability scan
- Functionality tests
- Edge case tests (from research)
- Performance tests

Usage:
    python test-fix.py --patch fix.patch --target file.py --test-suite tests.py --edge-cases research.md

Author: Intelligence Adjacent
Version: 1.0
Last Updated: 2025-11-24
"""

import argparse
import sys
import tempfile
import shutil
import subprocess
import json
import time
import re
from pathlib import Path
from typing import Dict, List, Any, Optional


# --- Environment Management ---

def create_test_environment(target_file: str) -> Path:
    """Create isolated test environment (temp directory with target copy)"""
    try:
        # Create temporary directory
        temp_dir = Path(tempfile.mkdtemp(prefix='remediation_test_'))

        # Copy target file to temp directory
        target_path = Path(target_file)
        if not target_path.exists():
            raise FileNotFoundError(f"Target file not found: {target_file}")

        dest_path = temp_dir / target_path.name
        shutil.copy2(target_path, dest_path)

        # Copy any supporting files in same directory (optional)
        parent_dir = target_path.parent
        for item in parent_dir.glob('*.py'):  # Copy all .py files for dependencies
            if item != target_path:
                shutil.copy2(item, temp_dir / item.name)

        print(f"[+] Created test environment: {temp_dir}")
        return temp_dir

    except Exception as e:
        print(f"[!] Failed to create test environment: {e}")
        sys.exit(1)


def cleanup_test_environment(test_env: Path):
    """Remove temporary test directory"""
    try:
        if test_env.exists():
            shutil.rmtree(test_env)
            print(f"[+] Cleaned up test environment: {test_env}")
    except Exception as e:
        print(f"[!] Warning: Failed to cleanup test environment: {e}")


# --- Patch Application ---

def apply_patch_to_test_copy(patch_file: str, test_env: Path, target_filename: str) -> bool:
    """Apply patch to test copy using Python's patch logic"""
    try:
        patch_path = Path(patch_file)
        if not patch_path.exists():
            raise FileNotFoundError(f"Patch file not found: {patch_file}")

        test_file = test_env / target_filename

        # Read patch file
        with open(patch_path, 'r', encoding='utf-8') as f:
            patch_content = f.read()

        # Simple patch application (assumes unified diff format)
        # For production, consider using 'patch' command or difflib

        # Try using system 'patch' command first
        try:
            result = subprocess.run(
                ['patch', '-p0', str(test_file)],
                input=patch_content,
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.returncode == 0:
                print(f"[+] Patch applied successfully using 'patch' command")
                return True
            else:
                print(f"[!] Patch command failed: {result.stderr}")
                return False

        except FileNotFoundError:
            # 'patch' command not available, try Python implementation
            print(f"[*] 'patch' command not found, using Python implementation")
            return apply_patch_python(patch_content, test_file)

    except Exception as e:
        print(f"[!] Failed to apply patch: {e}")
        return False


def apply_patch_python(patch_content: str, target_file: Path) -> bool:
    """Simple Python-based patch application (fallback)"""
    try:
        # Read original file
        with open(target_file, 'r', encoding='utf-8') as f:
            original_lines = f.readlines()

        # Parse patch (very simple parser for demonstration)
        # This assumes patches generated by generate-fix.py have clear markers

        # For MVP, we'll use a simple replacement approach
        # In production, use proper diff parsing library

        # Extract new version from patch if it contains full file replacement
        if '--- COMPLETE FILE ---' in patch_content:
            # Simple full file replacement
            new_content = patch_content.split('--- COMPLETE FILE ---')[1].strip()
            with open(target_file, 'w', encoding='utf-8') as f:
                f.write(new_content)
            print(f"[+] Applied full file replacement")
            return True
        else:
            print(f"[!] Patch format not recognized (expected unified diff or full file)")
            return False

    except Exception as e:
        print(f"[!] Python patch application failed: {e}")
        return False


# --- Test 1: Syntax Validation ---

def run_syntax_validation(test_file: Path) -> Dict[str, Any]:
    """Validate syntax (compile/run check)"""
    try:
        file_ext = test_file.suffix.lower()

        if file_ext == '.py':
            # Python syntax check
            with open(test_file, 'r', encoding='utf-8') as f:
                code = f.read()

            try:
                compile(code, str(test_file), 'exec')
                return {
                    'status': 'PASSED',
                    'message': 'Python syntax valid',
                    'details': 'Compilation successful'
                }
            except SyntaxError as e:
                return {
                    'status': 'FAILED',
                    'message': 'Python syntax error',
                    'details': str(e)
                }

        elif file_ext == '.php':
            # PHP syntax check
            result = subprocess.run(
                ['php', '-l', str(test_file)],
                capture_output=True,
                text=True,
                timeout=10
            )

            if result.returncode == 0:
                return {
                    'status': 'PASSED',
                    'message': 'PHP syntax valid',
                    'details': result.stdout.strip()
                }
            else:
                return {
                    'status': 'FAILED',
                    'message': 'PHP syntax error',
                    'details': result.stderr.strip()
                }

        elif file_ext in ['.js', '.ts']:
            # JavaScript/TypeScript syntax check using node
            result = subprocess.run(
                ['node', '--check', str(test_file)],
                capture_output=True,
                text=True,
                timeout=10
            )

            if result.returncode == 0:
                return {
                    'status': 'PASSED',
                    'message': f'{file_ext} syntax valid',
                    'details': 'Node check passed'
                }
            else:
                return {
                    'status': 'FAILED',
                    'message': f'{file_ext} syntax error',
                    'details': result.stderr.strip()
                }

        else:
            return {
                'status': 'SKIPPED',
                'message': f'No syntax validator for {file_ext}',
                'details': 'Manual review required'
            }

    except Exception as e:
        return {
            'status': 'ERROR',
            'message': 'Syntax validation failed',
            'details': str(e)
        }


# --- Test 2: Vulnerability Scan ---

def run_vulnerability_scan(test_file: Path, vuln_type: str) -> Dict[str, Any]:
    """Run original vulnerability test against fixed code"""
    try:
        file_ext = test_file.suffix.lower()

        if file_ext == '.py':
            # Use bandit for Python
            result = subprocess.run(
                ['bandit', '-f', 'json', str(test_file)],
                capture_output=True,
                text=True,
                timeout=30
            )

            try:
                bandit_output = json.loads(result.stdout)
                issues = bandit_output.get('results', [])

                # Filter for original vulnerability type
                relevant_issues = [
                    issue for issue in issues
                    if vuln_type.lower() in issue.get('issue_text', '').lower()
                ]

                if len(relevant_issues) == 0:
                    return {
                        'status': 'PASSED',
                        'message': f'No {vuln_type} vulnerabilities detected',
                        'details': f'Bandit found 0 instances of {vuln_type}'
                    }
                else:
                    return {
                        'status': 'FAILED',
                        'message': f'{vuln_type} still present',
                        'details': f'Found {len(relevant_issues)} instances'
                    }

            except json.JSONDecodeError:
                # Bandit not available or error
                return {
                    'status': 'SKIPPED',
                    'message': 'Bandit not available',
                    'details': 'Install bandit: pip install bandit'
                }

        else:
            return {
                'status': 'SKIPPED',
                'message': f'No vuln scanner configured for {file_ext}',
                'details': 'Manual testing required'
            }

    except FileNotFoundError:
        return {
            'status': 'SKIPPED',
            'message': 'Vulnerability scanner not found',
            'details': 'Install appropriate scanner (bandit, semgrep, etc.)'
        }
    except Exception as e:
        return {
            'status': 'ERROR',
            'message': 'Vulnerability scan failed',
            'details': str(e)
        }


# --- Test 3: Functionality Tests ---

def run_functionality_tests(test_file: Path, test_suite: str) -> Dict[str, Any]:
    """Run functionality tests to ensure feature still works"""
    try:
        if not test_suite or not Path(test_suite).exists():
            return {
                'status': 'SKIPPED',
                'message': 'No test suite provided',
                'details': 'Provide --test-suite for functionality testing'
            }

        # Copy test suite to test environment
        test_env = test_file.parent
        test_suite_path = Path(test_suite)
        test_suite_copy = test_env / test_suite_path.name
        shutil.copy2(test_suite_path, test_suite_copy)

        # Run pytest
        result = subprocess.run(
            ['pytest', str(test_suite_copy), '-v', '--tb=short'],
            capture_output=True,
            text=True,
            cwd=str(test_env),
            timeout=60
        )

        if result.returncode == 0:
            return {
                'status': 'PASSED',
                'message': 'All functionality tests passed',
                'details': result.stdout
            }
        else:
            return {
                'status': 'FAILED',
                'message': 'Some functionality tests failed',
                'details': result.stdout + '\n' + result.stderr
            }

    except FileNotFoundError:
        return {
            'status': 'SKIPPED',
            'message': 'pytest not found',
            'details': 'Install pytest: pip install pytest'
        }
    except Exception as e:
        return {
            'status': 'ERROR',
            'message': 'Functionality tests error',
            'details': str(e)
        }


# --- Test 4: Edge Case Tests ---

def extract_edge_cases_from_research(research_file: str) -> List[str]:
    """Extract edge cases from Grok analysis in research report"""
    try:
        research_path = Path(research_file)
        if not research_path.exists():
            return []

        with open(research_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Look for "Edge Cases" section in markdown
        edge_cases = []

        # Pattern: Look for bullet points under "Edge Cases" or "Adversarial" sections
        patterns = [
            r'## Edge Cases\s*\n(.*?)(?=\n##|\Z)',
            r'## Adversarial Analysis\s*\n(.*?)(?=\n##|\Z)',
            r'## What Could Go Wrong\s*\n(.*?)(?=\n##|\Z)'
        ]

        for pattern in patterns:
            matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
            for match in matches:
                # Extract bullet points
                lines = match.split('\n')
                for line in lines:
                    line = line.strip()
                    if line.startswith('- ') or line.startswith('* '):
                        edge_case = line[2:].strip()
                        if edge_case and len(edge_case) > 10:
                            edge_cases.append(edge_case)

        print(f"[+] Extracted {len(edge_cases)} edge cases from research")
        return edge_cases

    except Exception as e:
        print(f"[!] Failed to extract edge cases: {e}")
        return []


def run_edge_case_tests(test_file: Path, edge_cases: List[str]) -> Dict[str, Any]:
    """Run research-predicted edge case tests"""
    try:
        if not edge_cases:
            return {
                'status': 'SKIPPED',
                'message': 'No edge cases found in research',
                'details': 'No edge cases to test'
            }

        # For each edge case, create a basic test
        # This is a simplified version - production would need custom test generation

        test_results = []

        for i, edge_case in enumerate(edge_cases, 1):
            # Check if edge case mentions specific inputs
            # Example: "Unicode characters", "NULL bytes", "Long strings"

            if any(keyword in edge_case.lower() for keyword in ['unicode', 'utf', 'encoding']):
                test_results.append(f"Edge case {i}: Unicode test - NEEDS MANUAL VERIFICATION")
            elif any(keyword in edge_case.lower() for keyword in ['null', 'empty', 'blank']):
                test_results.append(f"Edge case {i}: Null/empty test - NEEDS MANUAL VERIFICATION")
            elif any(keyword in edge_case.lower() for keyword in ['long', 'large', 'overflow']):
                test_results.append(f"Edge case {i}: Length test - NEEDS MANUAL VERIFICATION")
            else:
                test_results.append(f"Edge case {i}: {edge_case[:50]}... - NEEDS MANUAL VERIFICATION")

        return {
            'status': 'PASSED',
            'message': f'Identified {len(edge_cases)} edge cases for testing',
            'details': '\n'.join(test_results),
            'note': 'Manual verification recommended for edge cases'
        }

    except Exception as e:
        return {
            'status': 'ERROR',
            'message': 'Edge case testing error',
            'details': str(e)
        }


# --- Test 5: Performance Test ---

def run_performance_test(original_file: str, test_file: Path) -> Dict[str, Any]:
    """Compare performance (before vs after)"""
    try:
        # Simple performance test: measure import/load time
        original_path = Path(original_file)

        if not original_path.exists():
            return {
                'status': 'SKIPPED',
                'message': 'Original file not accessible',
                'details': 'Cannot compare performance'
            }

        # For Python files, measure compilation time
        if test_file.suffix.lower() == '.py':
            # Time original
            start = time.time()
            with open(original_path, 'r', encoding='utf-8') as f:
                original_code = f.read()
            try:
                compile(original_code, str(original_path), 'exec')
                original_time = time.time() - start
            except:
                original_time = -1

            # Time test
            start = time.time()
            with open(test_file, 'r', encoding='utf-8') as f:
                test_code = f.read()
            try:
                compile(test_code, str(test_file), 'exec')
                test_time = time.time() - start
            except:
                test_time = -1

            if original_time > 0 and test_time > 0:
                diff_ms = (test_time - original_time) * 1000
                diff_pct = ((test_time - original_time) / original_time) * 100 if original_time > 0 else 0

                if abs(diff_pct) < 10:  # Less than 10% difference
                    return {
                        'status': 'PASSED',
                        'message': 'Performance impact minimal',
                        'details': f'Compile time difference: {diff_ms:.2f}ms ({diff_pct:+.1f}%)'
                    }
                else:
                    return {
                        'status': 'WARNING',
                        'message': 'Performance impact detected',
                        'details': f'Compile time difference: {diff_ms:.2f}ms ({diff_pct:+.1f}%)'
                    }
            else:
                return {
                    'status': 'SKIPPED',
                    'message': 'Performance test incomplete',
                    'details': 'Compilation failed for one or both files'
                }

        else:
            return {
                'status': 'SKIPPED',
                'message': 'Performance test not implemented for this file type',
                'details': 'Manual benchmarking recommended'
            }

    except Exception as e:
        return {
            'status': 'ERROR',
            'message': 'Performance test error',
            'details': str(e)
        }


# --- Test 6: Second-Order Injection ---

def run_second_order_test(test_file: Path) -> Dict[str, Any]:
    """Check for second-order injection vulnerabilities"""
    try:
        # Read file and check for data flow patterns
        with open(test_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # Simple heuristic checks
        concerns = []

        # Check for database writes followed by reads
        if 'INSERT' in content.upper() and 'SELECT' in content.upper():
            concerns.append('Database writeâ†’read pattern detected (manual review recommended)')

        # Check for session/cache writes
        if any(keyword in content.lower() for keyword in ['session', 'cache', 'redis', 'memcached']):
            if any(keyword in content.lower() for keyword in ['set', 'write', 'store']):
                concerns.append('Session/cache storage detected (verify retrieval is safe)')

        if concerns:
            return {
                'status': 'WARNING',
                'message': 'Potential second-order concerns',
                'details': '\n'.join(concerns)
            }
        else:
            return {
                'status': 'PASSED',
                'message': 'No obvious second-order patterns',
                'details': 'Data flow appears safe'
            }

    except Exception as e:
        return {
            'status': 'ERROR',
            'message': 'Second-order test error',
            'details': str(e)
        }


# --- Report Generation ---

def generate_test_report(results: Dict[str, Dict[str, Any]]) -> str:
    """Generate test results report"""
    report = []
    report.append("\n" + "="*70)
    report.append("TEST RESULTS REPORT")
    report.append("="*70)

    # Summary
    passed = sum(1 for r in results.values() if r.get('status') == 'PASSED')
    failed = sum(1 for r in results.values() if r.get('status') == 'FAILED')
    skipped = sum(1 for r in results.values() if r.get('status') == 'SKIPPED')
    warnings = sum(1 for r in results.values() if r.get('status') == 'WARNING')
    errors = sum(1 for r in results.values() if r.get('status') == 'ERROR')

    report.append(f"\nSummary: {passed} PASSED | {failed} FAILED | {warnings} WARNING | {skipped} SKIPPED | {errors} ERROR")
    report.append("")

    # Detailed results
    test_names = {
        'syntax': 'Test 1/6: Syntax Validation',
        'vuln_scan': 'Test 2/6: Vulnerability Scan',
        'functionality': 'Test 3/6: Functionality Tests',
        'edge_cases': 'Test 4/6: Edge Case Tests',
        'performance': 'Test 5/6: Performance Test',
        'second_order': 'Test 6/6: Second-Order Injection'
    }

    for key, result in results.items():
        test_name = test_names.get(key, key)
        status = result.get('status', 'UNKNOWN')
        message = result.get('message', '')
        details = result.get('details', '')

        # Status symbol
        if status == 'PASSED':
            symbol = '[+]'
        elif status == 'FAILED':
            symbol = '[!]'
        elif status == 'WARNING':
            symbol = '[~]'
        elif status == 'SKIPPED':
            symbol = '[-]'
        else:
            symbol = '[?]'

        report.append(f"{symbol} {test_name}")
        report.append(f"    Status: {status}")
        report.append(f"    {message}")
        if details and len(details) < 200:
            report.append(f"    Details: {details}")
        report.append("")

    report.append("="*70)

    return '\n'.join(report)


# --- Main ---

def main():
    parser = argparse.ArgumentParser(
        description='Test fix in isolated environment',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python test-fix.py --patch fix.patch --target vulnerable.py
  python test-fix.py --patch fix.patch --target app.py --test-suite tests/test_app.py
  python test-fix.py --patch fix.patch --target app.php --edge-cases research.md
        """
    )

    parser.add_argument('--patch', required=True, help='Patch file to test')
    parser.add_argument('--target', required=True, help='Target file to patch')
    parser.add_argument('--test-suite', help='Optional test suite (pytest format)')
    parser.add_argument('--edge-cases', help='Research file with edge cases')
    parser.add_argument('--vuln-type', default='sql_injection', help='Vulnerability type being fixed')

    args = parser.parse_args()

    # Validate inputs
    if not Path(args.patch).exists():
        print(f"[!] Patch file not found: {args.patch}")
        return 1

    if not Path(args.target).exists():
        print(f"[!] Target file not found: {args.target}")
        return 1

    print(f"\n[*] Starting isolated test environment...")
    print(f"[*] Target: {args.target}")
    print(f"[*] Patch: {args.patch}")

    # Create test environment
    test_env = create_test_environment(args.target)
    test_file = test_env / Path(args.target).name

    try:
        # Apply patch
        print(f"\n[*] Applying patch to test copy...")
        if not apply_patch_to_test_copy(args.patch, test_env, Path(args.target).name):
            print(f"[!] Failed to apply patch - aborting tests")
            return 1

        print(f"\n[*] Running validation tests...")
        results = {}

        # Test 1: Syntax
        print(f"\n[*] Test 1/6: Syntax validation...")
        results['syntax'] = run_syntax_validation(test_file)

        # Test 2: Vulnerability scan
        print(f"[*] Test 2/6: Vulnerability scan...")
        results['vuln_scan'] = run_vulnerability_scan(test_file, args.vuln_type)

        # Test 3: Functionality
        print(f"[*] Test 3/6: Functionality tests...")
        results['functionality'] = run_functionality_tests(test_file, args.test_suite)

        # Test 4: Edge cases
        print(f"[*] Test 4/6: Edge case tests...")
        if args.edge_cases:
            edge_cases = extract_edge_cases_from_research(args.edge_cases)
            results['edge_cases'] = run_edge_case_tests(test_file, edge_cases)
        else:
            results['edge_cases'] = {
                'status': 'SKIPPED',
                'message': 'No research file provided',
                'details': 'Use --edge-cases to enable'
            }

        # Test 5: Performance
        print(f"[*] Test 5/6: Performance test...")
        results['performance'] = run_performance_test(args.target, test_file)

        # Test 6: Second-order injection
        print(f"[*] Test 6/6: Second-order injection test...")
        results['second_order'] = run_second_order_test(test_file)

        # Generate report
        print(f"\n[*] Generating test report...")
        report = generate_test_report(results)
        print(report)

        # Save report to file
        report_file = test_env / 'test_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"\n[+] Full report saved: {report_file}")

        # Determine overall result
        critical_tests = ['syntax', 'vuln_scan']
        all_critical_passed = all(
            results[test].get('status') == 'PASSED'
            for test in critical_tests
            if test in results
        )

        no_failures = all(
            r.get('status') != 'FAILED'
            for r in results.values()
        )

        if all_critical_passed and no_failures:
            print(f"\n[+] All tests PASSED")
            print(f"[+] Safe to apply fix to production code")
            print(f"\n[*] Next step: python apply-fix.py --patch {args.patch} --target {args.target}")
            return 0
        else:
            print(f"\n[!] Some tests FAILED or had WARNINGS")
            print(f"[!] Review failures before applying to production")
            return 1

    finally:
        # Cleanup
        print(f"\n[*] Cleaning up test environment...")
        cleanup_test_environment(test_env)


if __name__ == '__main__':
    sys.exit(main())
