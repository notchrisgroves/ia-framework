# DSA-Compliant Notice & Action Policy Template
## Article 16: Internal Complaint-Handling & Reporting Mechanisms

**Policy Type:** Notice & Action Mechanism Policy
**DSA Articles Covered:** Article 16 (primary), also Article 22 (Internal complaint-handling systems)
**Target Services:** All online platforms (including VLOP/VLOSE)
**Template Version:** 1.0
**Last Updated:** 2025-12-02

---

## How to Use This Template

1. **Fill in all [bracketed] fields** with service-specific information
2. **Keep all 8 components** - required for DSA compliance
3. **Make reporting mechanism easy to find** - Maximum 2 clicks from any page
4. **Design for accessibility** - WCAG 2.1 AA compliant
5. **Translate to all 24 EU languages** before publication
6. **Review readability** - target Flesch-Kincaid grade level ≤10
7. **Legal review required** before publication
8. **Test the reporting workflow** - Ensure it works as documented

**DSA Requirements Checklist:**
- [ ] Reporting mechanism accessible on every page
- [ ] Easy-to-use interface (max 3 steps to submit)
- [ ] Electronic submission supported
- [ ] No account creation required to report (but can be optional)
- [ ] Trusted flagger program implemented (Article 22)
- [ ] Priority handling for trusted flaggers
- [ ] Decision within reasonable timeframe (specify in policy)
- [ ] Notifications sent to reporter and affected user
- [ ] Appeal mechanism available (see Article 20 template)

---

# [SERVICE NAME] Notice & Action Policy
## How to Report Content and Get Action

**Reporting Mechanism for EU Users**

---

## COMPONENT 1: Metadata

**Policy Version:** [Version number - e.g., 2.0]

**Effective Date:** [Date]

**Last Updated:** [Date]

**Next Scheduled Review:** [Date - recommended: quarterly]

**Responsible Owner:**
- **Executive Sponsor:** [C-Level - e.g., Chief Trust & Safety Officer]
- **Policy Owner:** [VP / Director of Trust & Safety]
- **Implementation Team:** [Trust & Safety, Product, Engineering, Legal]

**Policy Scope:** This policy applies to:
- **Users:** Anyone can report content (account not required)
- **Service:** [SERVICE NAME]
- **Content:** All user-generated content on the platform
- **Geographic Scope:** European Union (27 member states)

**Regulatory Basis:**
- EU Digital Services Act (Regulation 2022/2065), Article 16
- EUR-Lex Reference: https://eur-lex.europa.eu/eli/reg/2022/2065

**Change History:**
| Version | Date | Changes | Approver |
|---------|------|---------|----------|
| [1.0] | [Date] | Initial reporting mechanism policy | [Name] |
| [2.0] | [Date] | DSA Article 16 compliance updates | [Name] |

**Languages Available:**
This policy is available in all 24 official EU languages. See [URL] for translations.

---

## COMPONENT 2: Purpose & What You Can Report

### Policy Purpose

**Our Commitment:**

[SERVICE NAME] is committed to keeping our platform safe and lawful. We rely on our community to report content that violates:
- **EU or national laws** (illegal content)
- **Our Terms of Service** (content that violates our community standards)

This policy explains:
- **What you can report** (types of violations)
- **How to report** (easy, accessible reporting mechanism)
- **What happens next** (our review process and timelines)
- **How you'll be notified** (confirmation and outcome messages)
- **What if you disagree** (appeal process)

### What You Can Report

**You can report:**

☐ **Illegal Content** (violates EU or national law):
   - Child sexual abuse material (CSAM)
   - Terrorist content
   - Hate speech / Incitement to violence
   - Copyright infringement
   - Trademark infringement
   - Sale of illegal goods (drugs, weapons, counterfeits)
   - Fraud / Scams
   - Privacy violations / Doxing
   - Defamation
   - Other illegal content

☐ **Terms of Service Violations** (violates our community standards):
   - Spam
   - Harassment / Bullying
   - Graphic violence
   - Adult sexual content
   - Self-harm / Suicide content
   - Dangerous activities / Challenges
   - Misinformation (in certain categories)
   - Impersonation
   - Other violations: [LIST SERVICE-SPECIFIC VIOLATIONS]

☐ **Account Issues:**
   - Hacked account
   - Underage user (under minimum age)
   - Impersonation
   - Coordinated inauthentic behavior

☐ **Technical Issues:**
   - Content incorrectly removed (request review)
   - Account incorrectly suspended (see appeal process)
   - Bug in reporting system

### What You CANNOT Report Here

**For these issues, use different channels:**

- ❌ **Privacy / Data Requests:** Use [DATA REQUEST URL]
- ❌ **Copyright Takedown (DMCA):** Use [COPYRIGHT FORM URL]
- ❌ **General Customer Support:** Use [SUPPORT URL]
- ❌ **Account Recovery:** Use [ACCOUNT RECOVERY URL]
- ❌ **Advertising Issues:** Use [AD COMPLAINTS URL]

---

## COMPONENT 3: How to Report (Step-by-Step)

### Option 1: In-App Reporting (Fastest - Recommended)

**On Desktop/Web:**

1. **Find the content** you want to report (post, comment, video, profile, ad, etc.)
2. **Click the [ICON]** (e.g., three dots, flag icon, "More" button)
3. **Select "Report"**
4. **Choose category:**
   - Illegal content
   - Terms of Service violation
   - Other (specify)
5. **Select specific violation type:**
   - [DROPDOWN LIST - e.g., "Hate speech," "Scam," "Child safety," etc.]
6. **Add details (optional but helpful):**
   - Explain why this violates [LAW / TERMS]
   - Provide context (if needed)
   - Add screenshots (if you have external evidence)
7. **Submit report**

**On Mobile App:**

1. **Tap the content** you want to report
2. **Tap [ICON]** (e.g., three dots, flag icon)
3. **Tap "Report"**
4. **Select category** (same as desktop)
5. **Select violation type**
6. **Add details** (optional)
7. **Submit**

**You'll Receive:**
- ✅ **Instant confirmation:** "Report received - Case #[NUMBER]"
- ✅ **Email confirmation:** Sent to [YOUR EMAIL if you provide one]
- ✅ **Status updates:** You can check report status at [URL]

---

### Option 2: Web Form Reporting (For Complex Reports)

**When to use:** If you need to report multiple items, provide extensive evidence, or report something not easily found on the platform.

**How:**

1. **Go to:** [URL - e.g., "service.com/report"]
2. **Fill in the form:**
   - **Your Contact Info (optional):**
     - Email: [OPTIONAL - for updates]
     - Name: [OPTIONAL]
   - **What You're Reporting:**
     - Content URL(s): [PASTE LINKS]
     - Account/Profile: [USERNAME / URL]
     - Violation type: [DROPDOWN]
   - **Why It Violates:**
     - Explanation: [TEXT BOX - max [NUMBER] characters]
     - Evidence: [UPLOAD FILES - screenshots, documents, URLs]
   - **Legal Basis (if reporting illegal content):**
     - Which law: [e.g., "German NetzDG §1," "EU Copyright Directive," etc.]
     - Why illegal: [EXPLANATION]
3. **Submit report**
4. **Receive confirmation:** Case #[NUMBER] and email confirmation

---

### Option 3: Email Reporting (For Serious Issues / Emergencies)

**When to use:** Urgent child safety issues, ongoing harassment, imminent threats.

**Email:** [REPORT EMAIL - e.g., "abuse@service.com," "childsafety@service.com"]

**Subject Line:** [SPECIFY FORMAT - e.g., "URGENT: [Violation Type] Report"]

**Include:**
- **What:** URL of content/account
- **Why:** Violation type and explanation
- **Evidence:** Screenshots, links, context
- **Your Contact:** Email (optional for updates)

**Response Time:** [TIMEFRAME - e.g., "Emergency reports within 1 hour, others within 24 hours"]

---

### Option 4: No Account Required

**You do NOT need a [SERVICE NAME] account to report content.**

**Anonymous Reporting:**
- Go to [REPORTING URL]
- Fill in report form (no login required)
- Provide email (optional - for updates only)

**Note:** We take all reports seriously, whether from account holders or anonymous reporters.

---

## COMPONENT 4: What Happens After You Report

### Our Review Process

```
┌────────────────────────────────────────────────────────┐
│               REPORT REVIEW WORKFLOW                   │
└────────────────────────────────────────────────────────┘

STEP 1: REPORT RECEIVED
   │
   └─→ You get instant confirmation: Case #[NUMBER]
       Email sent (if you provided email)

STEP 2: TRIAGE (Within [TIMEFRAME - e.g., "15 minutes"])
   │
   ├─→ PRIORITY FLAGGING:
   │   ├─ CSAM / Terrorism → Immediate review
   │   ├─ Imminent threats → Review within 1 hour
   │   ├─ Illegal content → Review within 24 hours
   │   └─ Terms violations → Review within [TIMEFRAME]
   │
   └─→ TRUSTED FLAGGER CHECK:
       └─ Report from trusted flagger? → Priority review

STEP 3: REVIEW (By Human Moderator)
   │
   ├─→ CONTENT ASSESSMENT:
   │   ├─ Does it violate [LAW / TERMS]?
   │   ├─ Context considered (satire, news, education?)
   │   └─ Evidence reviewed (your explanation + our investigation)
   │
   └─→ DECISION:
       ├─ REMOVE: Content violates law/terms → Taken down
       ├─ RESTRICT: Content limited (age-gate, warning label)
       ├─ NO ACTION: Content does not violate → Stays up
       └─ ESCALATE: Borderline case → Senior reviewer

STEP 4: ACTION (If Violation Confirmed)
   │
   ├─→ CONTENT ACTIONS:
   │   ├─ Remove from platform
   │   ├─ Restrict visibility (age-gate, country-block)
   │   ├─ Add warning label
   │   └─ Demote in recommendations
   │
   ├─→ ACCOUNT ACTIONS:
   │   ├─ Warning (first offense)
   │   ├─ Temporary suspension
   │   ├─ Permanent termination
   │   └─ Feature restrictions (no posting, no comments)
   │
   └─→ LAW ENFORCEMENT (If Illegal Content):
       └─ Report to [AUTHORITY - e.g., NCMEC, local police]

STEP 5: NOTIFICATIONS
   │
   ├─→ TO REPORTER (You):
   │   ├─ "Action taken" (content removed/restricted)
   │   ├─ "No action taken" (does not violate)
   │   └─ "More info needed" (rare - we may contact you)
   │
   └─→ TO CONTENT OWNER:
       └─ Statement of Reasons (see Article 20 Policy)
           ├─ What we did
           ├─ Why (legal basis, facts)
           ├─ How to appeal
           └─ Redress options
```

### Review Timeframes

**We aim to review reports within:**

| Violation Type | Review Timeframe | Action Timeframe |
|----------------|------------------|------------------|
| **CSAM** | Immediate | Within 1 hour |
| **Terrorist Content** | Within 1 hour | Within 1 hour |
| **Imminent Threats** | Within 1 hour | Within 1 hour |
| **Illegal Content** | Within 24 hours | Within 24 hours |
| **Serious Harms (child safety, violence)** | Within 24 hours | Within 48 hours |
| **Terms Violations** | Within [TIMEFRAME - e.g., "3 days"] | Within [TIMEFRAME] |
| **Other Issues** | Within [TIMEFRAME - e.g., "7 days"] | Within [TIMEFRAME] |

**Trusted Flagger Reports:** Reviewed [X] times faster than standard reports.

**Note:** These are target timeframes. Complex cases may take longer. You'll be notified of any delays.

### What You'll Be Notified About

**You (The Reporter) Receive:**

1. **Immediate:** Confirmation that we received your report
2. **When Reviewed:** Outcome of review
   - ✅ "Action taken: Content removed/restricted"
   - ⏹️ "No action taken: Does not violate [law/terms]"
   - ❓ "More information needed" (rare)
3. **Status Updates:** If review takes longer than [TIMEFRAME], we send status update

**Content Owner Receives:**

- **Statement of Reasons** (if content removed/restricted) - See Article 20 Policy
- **Appeal Information** - How to challenge our decision

---

## COMPONENT 5: Trusted Flagger Program (Article 22)

**DSA Article 22: Platforms must have trusted flagger programs**

### What is a Trusted Flagger?

**Trusted Flaggers** are organizations with proven expertise in detecting specific types of illegal content. They receive:
- ✅ **Priority review** - Reports processed [X] times faster
- ✅ **Direct contact** - Dedicated support channel
- ✅ **Batch reporting** - Can report multiple items at once
- ✅ **Transparency** - Statistics on report accuracy and actions taken

**Trusted Flaggers are NOT:**
- ❌ Individual users (only organizations)
- ❌ Able to directly remove content (we still review)
- ❌ Paid by [SERVICE NAME] (independent organizations)

### Who are Our Trusted Flaggers?

**Current Trusted Flaggers for [SERVICE NAME]:**

| Organization | Specialization | Contact | Status |
|--------------|----------------|---------|--------|
| [ORG NAME 1] | CSAM detection | [EMAIL] | Active |
| [ORG NAME 2] | Terrorist content | [EMAIL] | Active |
| [ORG NAME 3] | Hate speech | [EMAIL] | Active |
| [ORG NAME 4] | Intellectual property | [EMAIL] | Active |
| [ORG NAME 5] | Consumer protection | [EMAIL] | Active |

**Full List:** [URL]

### How to Become a Trusted Flagger

**Eligibility Criteria:**

1. **Organization Type:**
   - ☐ NGO / Non-profit
   - ☐ Government body (police, regulatory authority)
   - ☐ Semi-governmental body (with legal authority)

2. **Expertise:**
   - ☐ Proven track record in detecting [CONTENT TYPE]
   - ☐ At least [NUMBER - e.g., "2 years"] experience
   - ☐ Staff trained in relevant laws and policies

3. **Independence:**
   - ☐ Not owned or controlled by [SERVICE NAME]
   - ☐ No conflicts of interest

4. **Accountability:**
   - ☐ Transparent governance
   - ☐ Publicly available information about organization
   - ☐ Willingness to publish annual reports on reporting activity

**Application Process:**

1. Submit application at [URL]
2. Provide: Organization details, expertise evidence, references
3. We review within [TIMEFRAME - e.g., "30 days"]
4. If approved: Training, onboarding, access to trusted flagger portal
5. Performance review: [FREQUENCY - e.g., "Quarterly"]
   - Accuracy rate must be ≥[PERCENTAGE - e.g., "80%"]
   - False positives monitored
   - Status can be revoked if standards not met

---

## COMPONENT 6: Reporting by Authorities (Article 11)

**DSA Article 11: Law enforcement and government authorities have special reporting channels**

### Expedited Reporting for Authorities

**Who Qualifies:**
- Law enforcement agencies (police, prosecutors)
- Regulatory authorities (consumer protection, data protection)
- Other governmental bodies with legal mandate

**What's Different:**
- ✅ **Priority processing** - Faster than standard reports
- ✅ **Direct contact** - Dedicated liaison team
- ✅ **Legal orders portal** - Submit orders electronically
- ✅ **Status tracking** - Real-time updates on compliance

**Contact:**
- **Email:** [AUTHORITY EMAIL - e.g., "legal@service.com," "lawenforcement@service.com"]
- **Portal:** [URL]
- **Phone:** [NUMBER] ([HOURS - e.g., "24/7 for emergencies"])

**Response Time:** [TIMEFRAME - e.g., "Within 24 hours, within 1 hour for emergencies (CSAM, imminent threats)"]

---

## COMPONENT 7: Reporter Protection & Confidentiality

### Your Privacy as a Reporter

**What We Collect When You Report:**

- **If you provide:** Your email address (optional)
- **Automatically:** Report timestamp, content reported, violation type selected
- **NOT collected:** Your IP address, browsing history, or other personal data (unless required by law for criminal investigations)

**How We Use Your Information:**

- ✅ Process your report
- ✅ Send you updates (if you provided email)
- ✅ Improve our content moderation systems (anonymized aggregate data)
- ❌ **We DO NOT:** Share your identity with the content owner
- ❌ **We DO NOT:** Use your data for advertising or other purposes

**Anonymous Reporting:**

- You can report without providing any contact info
- You won't receive updates, but we still process your report

### Protection from Retaliation

**We prohibit retaliation against reporters:**

- ❌ **Content owners CANNOT:**
  - See who reported their content
  - Retaliate against reporters (if identity somehow discovered)
  - File false counter-reports

**If you experience retaliation:**
- Report to us at [EMAIL]
- We investigate and may [ACTIONS - e.g., "suspend/terminate retaliator's account"]

### Legal Disclosure

**We may disclose reporter information ONLY if:**
- Required by law (court order, criminal investigation)
- Reporter consents
- To prevent imminent harm (life-threatening situations)

**We will notify you** if we receive a legal request for your information (unless prohibited by law).

---

## COMPONENT 8: Transparency & Accountability

### Transparency Reporting (Article 24)

**We publish transparency reports every 6 months at [URL]:**

**What's Included:**

- **Reports Received:** Total number, by violation type
- **Actions Taken:**
  - Content removed: [NUMBER]
  - Content restricted: [NUMBER]
  - No action taken: [NUMBER]
- **Review Timeframes:** Average time to review and act
- **Trusted Flagger Reports:** Number received, accuracy rate
- **Authority Reports:** Number from law enforcement/regulators
- **Appeal Outcomes:** How many decisions were reversed on appeal

**Breakdown by:**
- Content type (video, text, image, etc.)
- Violation type (illegal content, terms violations)
- Detection method (user report, automated detection, trusted flagger)

### Quality Assurance

**How we ensure quality decisions:**

1. **Training:** All moderators trained on [FREQUENCY - e.g., "quarterly"] on:
   - Relevant laws (EU and national)
   - Our Terms of Service
   - Cultural context and nuance
   - Bias prevention

2. **Quality Checks:**
   - [PERCENTAGE - e.g., "10%"] of decisions randomly reviewed by senior moderators
   - Consistent error rate monitoring
   - Regular policy updates based on quality findings

3. **Appeal Review:**
   - Different moderator reviews appeals (not the original decision-maker)
   - Reversal rate tracked and analyzed
   - Policies updated if high reversal rate on specific issue

### Feedback & Improvement

**Help us improve:**

**Feedback on Reporting Process:**
- After submitting report, you can rate: "Was this easy to use?"
- Submit suggestions at [URL]

**Feedback on Decisions:**
- If you disagree with our decision, file an appeal (see Article 20 Policy)
- Explain why you think we got it wrong
- We review and may reverse

---

## Template Notes for Compliance Teams

### DSA Article 16 Requirements Met

This template ensures compliance with all DSA Article 16 requirements:

1. ✅ **Easy-to-Access Mechanism** (Component 3 - max 2 clicks)
2. ✅ **Electronic Submission** (In-app, web form, email)
3. ✅ **No Account Required** (Anonymous reporting option)
4. ✅ **Specific Violations** (Clear categories in reporting flow)
5. ✅ **Illegal Content Priority** (Component 4 - faster review for illegal content)
6. ✅ **Notifications** (Reporter + content owner notified)
7. ✅ **Statement of Reasons** (Cross-reference to Article 20 template)

### DSA Article 22 Requirements Met (Trusted Flaggers)

1. ✅ **Trusted Flagger Program** (Component 5)
2. ✅ **Priority Processing** (Faster review for trusted flaggers)
3. ✅ **Eligibility Criteria** (Clear standards)
4. ✅ **Transparency** (Public list of trusted flaggers)

### Customization Guidelines

**For Different Service Types:**

- **Social Media:** Emphasize harassment, hate speech, misinformation reporting
- **Video Platforms:** Focus on copyright, child safety, violent content
- **Marketplaces:** Highlight illegal goods, scams, counterfeit products
- **App Stores:** Emphasize malicious apps, misleading listings
- **Search Engines:** Focus on illegal content ranking, misleading results

**Reporting UX Best Practices:**

- **Max 3 clicks** from content to report submission
- **Mobile-optimized** (most reports come from mobile)
- **Pre-filled fields** (URL, content ID auto-filled when reporting from content)
- **Clear categories** (not overwhelming dropdown with 100 options)
- **Optional details** (allow quick reports without forcing users to write essays)

### Quality Assurance Checklist

Before publishing notice & action policy:

- [ ] All [bracketed] fields filled in
- [ ] Reporting mechanism implemented and accessible on every page
- [ ] Reporting flow tested (max 3 steps from any content to submission)
- [ ] Review timeframes are realistic (can you meet them?)
- [ ] Trusted flagger program active (at least 3 trusted flaggers)
- [ ] Authority reporting channel functional
- [ ] Notification system working (reporters + content owners get messages)
- [ ] Statement of Reasons template ready (Article 20)
- [ ] Transparency reporting system ready (Article 24)
- [ ] Legal review completed
- [ ] Translated to all 24 EU languages
- [ ] WCAG 2.1 AA accessibility compliance verified

### Common Mistakes to Avoid

❌ **Don't:** Hide reporting button in obscure menu
✅ **Do:** Make it visible on every page (e.g., footer, "Report" button on all content)

❌ **Don't:** Require account creation to report
✅ **Do:** Allow anonymous reporting (email optional for updates)

❌ **Don't:** Use vague violation categories like "inappropriate content"
✅ **Do:** Use specific, clear categories (hate speech, scam, child safety, etc.)

❌ **Don't:** Promise review within "reasonable time"
✅ **Do:** Specify exact timeframes (e.g., "24 hours for illegal content")

❌ **Don't:** Forget to notify reporters of outcomes
✅ **Do:** Always send confirmation + outcome message (action taken or not)

❌ **Don't:** Share reporter identity with content owner
✅ **Do:** Protect reporter confidentiality (prevents retaliation)

---

## EUR-Lex References

**Primary Regulation:**
- **EU Digital Services Act** (Regulation 2022/2065)
- **Full Regulation:** https://eur-lex.europa.eu/eli/reg/2022/2065

**DSA Article 16 - Notice & Action Mechanisms:**
- **Article 16 Text:** https://eur-lex.europa.eu/eli/reg/2022/2065/oj#d1e3080-1-1
- **Article 16(1):** Obligation to provide easy-to-access and user-friendly reporting mechanism
- **Article 16(2):** Minimum information required in reports
- **Article 16(5):** Decision and notification requirements

**DSA Article 22 - Trusted Flaggers:**
- **Article 22 Text:** https://eur-lex.europa.eu/eli/reg/2022/2065/oj#d1e3375-1-1
- **Article 22(1):** Status designation by Digital Services Coordinator
- **Article 22(2):** Priority handling obligation

**DSA Article 11 - Authority Reporting:**
- **Article 11 Text:** https://eur-lex.europa.eu/eli/reg/2022/2065/oj#d1e2570-1-1
- Direct reporting channel for law enforcement and regulatory authorities

**Related DSA Articles:**
- **Article 20:** Statement of Reasons - https://eur-lex.europa.eu/eli/reg/2022/2065/oj#d1e3212-1-1
- **Article 24:** Transparency reporting - https://eur-lex.europa.eu/eli/reg/2022/2065/oj#d1e3511-1-1

---

**Template Version:** 1.0
**Created:** 2025-12-02
**Status:** Production-ready
**Maintained By:** DSA Compliance Team + Trust & Safety Team
**Next Review:** Quarterly or upon workflow changes
